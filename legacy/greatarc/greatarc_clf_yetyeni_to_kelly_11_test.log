train: (1204, 27430)
test: (1894, 27430)
Positive labels in train:  69
Positive labels in test:  212
Iteration 1, loss = 56.37846876
Iteration 2, loss = 51.28023938
Iteration 3, loss = 48.07984891
Iteration 4, loss = 45.00866547
Iteration 5, loss = 42.10953603
Iteration 6, loss = 39.38793016
Iteration 7, loss = 36.84001031
Iteration 8, loss = 34.45681927
Iteration 9, loss = 32.22337707
Iteration 10, loss = 30.12949356
Iteration 11, loss = 28.16838049
Iteration 12, loss = 26.33051970
Iteration 13, loss = 24.60912339
Iteration 14, loss = 22.99846177
Iteration 15, loss = 21.48828594
Iteration 16, loss = 20.07098185
Iteration 17, loss = 18.74217289
Iteration 18, loss = 17.49801239
Iteration 19, loss = 16.33412689
Iteration 20, loss = 15.24632086
Iteration 21, loss = 14.22176396
Iteration 22, loss = 13.26464348
Iteration 23, loss = 12.37153854
Iteration 24, loss = 11.53716341
Iteration 25, loss = 10.75185849
Iteration 26, loss = 10.02035628
Iteration 27, loss = 9.33620634
Iteration 28, loss = 8.69837620
Iteration 29, loss = 8.10415741
Iteration 30, loss = 7.54915227
Iteration 31, loss = 7.03270474
Iteration 32, loss = 6.55137706
Iteration 33, loss = 6.10263599
Iteration 34, loss = 5.68451010
Iteration 35, loss = 5.29568076
Iteration 36, loss = 4.93359121
Iteration 37, loss = 4.59800586
Iteration 38, loss = 4.28686805
Iteration 39, loss = 3.99787057
Iteration 40, loss = 3.72877489
Iteration 41, loss = 3.48302446
Iteration 42, loss = 3.26185359
Iteration 43, loss = 3.04376135
Iteration 44, loss = 2.84391463
Iteration 45, loss = 2.65750254
Iteration 46, loss = 2.48715513
Iteration 47, loss = 2.33139094
Iteration 48, loss = 2.18780415
Iteration 49, loss = 2.05475662
Iteration 50, loss = 1.93186950
Iteration 51, loss = 1.81915888
Iteration 52, loss = 1.71918997
Iteration 53, loss = 1.62773118
Iteration 54, loss = 1.53449891
Iteration 55, loss = 1.45081773
Iteration 56, loss = 1.37495383
Iteration 57, loss = 1.30644243
Iteration 58, loss = 1.24344129
Iteration 59, loss = 1.18821260
Iteration 60, loss = 1.13612313
Iteration 61, loss = 1.08583690
Iteration 62, loss = 1.04113088
Iteration 63, loss = 1.00179616
Iteration 64, loss = 0.96481919
Iteration 65, loss = 0.93220563
Iteration 66, loss = 0.90053249
Iteration 67, loss = 0.87413195
Iteration 68, loss = 0.86088052
Iteration 69, loss = 0.82520719
Iteration 70, loss = 0.80299571
Iteration 71, loss = 0.78228286
Iteration 72, loss = 0.76447693
Iteration 73, loss = 0.74900750
Iteration 74, loss = 0.73497261
Iteration 75, loss = 0.72172751
Iteration 76, loss = 0.71548703
Iteration 77, loss = 0.70500261
Iteration 78, loss = 0.69033772
Iteration 79, loss = 0.67852093
Iteration 80, loss = 0.66842598
Iteration 81, loss = 0.66003825
Iteration 82, loss = 0.65453701
Iteration 83, loss = 0.64694829
Iteration 84, loss = 0.63836900
Iteration 85, loss = 0.63161969
Iteration 86, loss = 0.62679198
Iteration 87, loss = 0.62229018
Iteration 88, loss = 0.61784195
Iteration 89, loss = 0.61410485
Iteration 90, loss = 0.61080905
Iteration 91, loss = 0.60682127
Iteration 92, loss = 0.60414756
Iteration 93, loss = 0.60142106
Iteration 94, loss = 0.59774805
Iteration 95, loss = 0.59710693
Iteration 96, loss = 0.60658396
Iteration 97, loss = 0.60415654
Iteration 98, loss = 0.59499069
Iteration 99, loss = 0.58844982
Iteration 100, loss = 0.58469539
Iteration 101, loss = 0.58096332
Iteration 102, loss = 0.57948247
Iteration 103, loss = 0.57927648
Iteration 104, loss = 0.57792839
Iteration 105, loss = 0.59758526
Iteration 106, loss = 0.58829841
Iteration 107, loss = 0.57692308
Iteration 108, loss = 0.57777138
Iteration 109, loss = 0.57166737
Iteration 110, loss = 0.56732455
Iteration 111, loss = 0.57427602
Iteration 112, loss = 0.57446290
Iteration 113, loss = 0.56869079
Iteration 114, loss = 0.56790526
Iteration 115, loss = 0.56577045
Iteration 116, loss = 0.56177903
Iteration 117, loss = 0.55894683
Iteration 118, loss = 0.55641737
Iteration 119, loss = 0.55548461
Iteration 120, loss = 0.56021009
Iteration 121, loss = 0.56291918
Iteration 122, loss = 0.56640354
Iteration 123, loss = 0.55736688
Iteration 124, loss = 0.55168766
Iteration 125, loss = 0.54853134
Iteration 126, loss = 0.55125395
Iteration 127, loss = 0.55081431
Iteration 128, loss = 0.54685827
Iteration 129, loss = 0.54379229
Iteration 130, loss = 0.54966681
Iteration 131, loss = 0.55099024
Iteration 132, loss = 0.54656251
Iteration 133, loss = 0.54231423
Iteration 134, loss = 0.54158238
Iteration 135, loss = 0.54289482
Iteration 136, loss = 0.54011633
Iteration 137, loss = 0.53784823
Iteration 138, loss = 0.53809860
Iteration 139, loss = 0.53860677
Iteration 140, loss = 0.53502621
Iteration 141, loss = 0.53295321
Iteration 142, loss = 0.53184173
Iteration 143, loss = 0.53084278
Iteration 144, loss = 0.52898639
Iteration 145, loss = 0.52758275
Iteration 146, loss = 0.52677724
Iteration 147, loss = 0.52567115
Iteration 148, loss = 0.52387384
Iteration 149, loss = 0.52356740
Iteration 150, loss = 0.52214051
Iteration 151, loss = 0.52119272
Iteration 152, loss = 0.52086469
Iteration 153, loss = 0.52114731
Iteration 154, loss = 0.52039811
Iteration 155, loss = 0.52218670
Iteration 156, loss = 0.59207449
Iteration 157, loss = 0.54301400
Iteration 158, loss = 0.52779435
Iteration 159, loss = 0.51825884
Iteration 160, loss = 0.51167704
Iteration 161, loss = 0.51151101
Iteration 162, loss = 0.53687863
Iteration 163, loss = 0.52090545
Iteration 164, loss = 0.51458427
Iteration 165, loss = 0.51986073
Iteration 166, loss = 0.51816140
Iteration 167, loss = 0.51031068
Iteration 168, loss = 0.50626275
Iteration 169, loss = 0.50335246
Iteration 170, loss = 0.50106909
Iteration 171, loss = 0.50022606
Iteration 172, loss = 0.51174310
Iteration 173, loss = 0.50868261
Iteration 174, loss = 0.50397982
Iteration 175, loss = 0.50221277
Iteration 176, loss = 0.50162098
Iteration 177, loss = 0.49876347
Iteration 178, loss = 0.49718388
Iteration 179, loss = 0.49503292
Iteration 180, loss = 0.49292094
Iteration 181, loss = 0.49121040
Iteration 182, loss = 0.49070461
Iteration 183, loss = 0.49102045
Iteration 184, loss = 0.49313187
Iteration 185, loss = 0.49184763
Iteration 186, loss = 0.48914036
Iteration 187, loss = 0.48620858
Iteration 188, loss = 0.48527106
Iteration 189, loss = 0.48524795
Iteration 190, loss = 0.48554029
Iteration 191, loss = 0.48489042
Iteration 192, loss = 0.48397587
Iteration 193, loss = 0.50511029
Iteration 194, loss = 0.49545011
Iteration 195, loss = 0.48852428
Iteration 196, loss = 0.48070626
Iteration 197, loss = 0.47942096
Iteration 198, loss = 0.47723759
Iteration 199, loss = 0.47811283
Iteration 200, loss = 0.50028691
/opt/anaconda3/envs/audio_clf/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(
-----training-----
precision_score:  0.8627450980392157
recall_score:  0.6376811594202898
f1_score:  0.7333333333333333
accuracy_score:  0.973421926910299
-----testing-----
precision_score:  0.5271966527196653
recall_score:  0.5943396226415094
f1_score:  0.5587583148558759
accuracy_score:  0.8949313621964097
--- 307.52795791625977 seconds ---