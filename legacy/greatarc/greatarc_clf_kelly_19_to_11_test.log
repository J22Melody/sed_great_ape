train: (3963, 27430)
test: (1894, 27430)
Positive labels in train:  312
Positive labels in test:  212
Iteration 1, loss = 44.37402158
Iteration 2, loss = 30.96378247
Iteration 3, loss = 20.86654727
Iteration 4, loss = 13.87834213
Iteration 5, loss = 9.10353539
Iteration 6, loss = 5.93144581
Iteration 7, loss = 3.87460422
Iteration 8, loss = 2.56857212
Iteration 9, loss = 1.75753794
Iteration 10, loss = 1.26295077
Iteration 11, loss = 0.96458788
Iteration 12, loss = 0.79761013
Iteration 13, loss = 0.69697032
Iteration 14, loss = 0.62693697
Iteration 15, loss = 0.58513116
Iteration 16, loss = 0.55768716
Iteration 17, loss = 0.54052140
Iteration 18, loss = 0.53156829
Iteration 19, loss = 0.52295210
Iteration 20, loss = 0.51483804
Iteration 21, loss = 0.50671037
Iteration 22, loss = 0.49992378
Iteration 23, loss = 0.49285397
Iteration 24, loss = 0.48616871
Iteration 25, loss = 0.48319961
Iteration 26, loss = 0.49400541
Iteration 27, loss = 0.49451274
Iteration 28, loss = 0.47882393
Iteration 29, loss = 0.46480342
Iteration 30, loss = 0.46255232
Iteration 31, loss = 0.45657093
Iteration 32, loss = 0.45353313
Iteration 33, loss = 0.46226907
Iteration 34, loss = 0.48508903
Iteration 35, loss = 0.47239828
Iteration 36, loss = 0.44900153
Iteration 37, loss = 0.43799990
Iteration 38, loss = 0.42830279
Iteration 39, loss = 0.42391987
Iteration 40, loss = 0.42095473
Iteration 41, loss = 0.42227064
Iteration 42, loss = 0.41556757
Iteration 43, loss = 0.41269464
Iteration 44, loss = 0.40828454
Iteration 45, loss = 0.40954815
Iteration 46, loss = 0.45808279
Iteration 47, loss = 0.45422433
Iteration 48, loss = 0.41298057
Iteration 49, loss = 0.39913389
Iteration 50, loss = 0.39246924
Iteration 51, loss = 0.38900435
Iteration 52, loss = 0.38606478
Iteration 53, loss = 0.38429679
Iteration 54, loss = 0.38192758
Iteration 55, loss = 0.37904847
Iteration 56, loss = 0.37680373
Iteration 57, loss = 0.37486777
Iteration 58, loss = 0.37503242
Iteration 59, loss = 0.37433645
Iteration 60, loss = 0.37322532
Iteration 61, loss = 0.36966349
Iteration 62, loss = 0.36654764
Iteration 63, loss = 0.36486152
Iteration 64, loss = 0.36503846
Iteration 65, loss = 0.36221017
Iteration 66, loss = 0.35926600
Iteration 67, loss = 0.35841960
Iteration 68, loss = 0.35779372
Iteration 69, loss = 0.35670825
Iteration 70, loss = 0.35401063
Iteration 71, loss = 0.35268427
Iteration 72, loss = 0.35205719
Iteration 73, loss = 0.34948585
Iteration 74, loss = 0.38518379
Iteration 75, loss = 0.39729253
Iteration 76, loss = 0.35909116
Iteration 77, loss = 0.34970897
Iteration 78, loss = 0.34607790
Iteration 79, loss = 0.34442773
Iteration 80, loss = 0.34505004
Iteration 81, loss = 0.34252019
Iteration 82, loss = 0.33847326
Iteration 83, loss = 0.33860588
Iteration 84, loss = 0.33721173
Iteration 85, loss = 0.33457299
Iteration 86, loss = 0.33516798
Iteration 87, loss = 0.33368221
Iteration 88, loss = 0.33365590
Iteration 89, loss = 0.33669225
Iteration 90, loss = 0.33181655
Iteration 91, loss = 0.33034460
Iteration 92, loss = 0.32949486
Iteration 93, loss = 0.33030103
Iteration 94, loss = 0.32752811
Iteration 95, loss = 0.32635034
Iteration 96, loss = 0.32558906
Iteration 97, loss = 0.32654828
Iteration 98, loss = 0.32498006
Iteration 99, loss = 0.32411422
Iteration 100, loss = 0.32270073
Iteration 101, loss = 0.32250944
Iteration 102, loss = 0.32188092
Iteration 103, loss = 0.32071850
Iteration 104, loss = 0.31924347
Iteration 105, loss = 0.31739584
Iteration 106, loss = 0.31847818
Iteration 107, loss = 0.31986484
Iteration 108, loss = 0.32415530
Iteration 109, loss = 0.31835587
Iteration 110, loss = 0.31465269
Iteration 111, loss = 0.31888227
Iteration 112, loss = 0.31722419
Iteration 113, loss = 0.31684620
Iteration 114, loss = 0.32692900
Iteration 115, loss = 0.31775586
Iteration 116, loss = 0.31415806
Iteration 117, loss = 0.31375906
Iteration 118, loss = 0.31256355
Iteration 119, loss = 0.31170646
Iteration 120, loss = 0.31011892
Iteration 121, loss = 0.31037284
Iteration 122, loss = 0.30964087
Iteration 123, loss = 0.30779582
Iteration 124, loss = 0.30841066
Iteration 125, loss = 0.30801012
Iteration 126, loss = 0.30869810
Iteration 127, loss = 0.30847576
Iteration 128, loss = 0.30653291
Iteration 129, loss = 0.30654960
Iteration 130, loss = 0.30472206
Iteration 131, loss = 0.30600851
Iteration 132, loss = 0.30567657
Iteration 133, loss = 0.30519276
Iteration 134, loss = 0.30410028
Iteration 135, loss = 0.30488659
Iteration 136, loss = 0.30552537
Iteration 137, loss = 0.30411213
Iteration 138, loss = 0.30230966
Iteration 139, loss = 0.30895521
Iteration 140, loss = 0.30278407
Iteration 141, loss = 0.30337031
Iteration 142, loss = 0.30515074
Iteration 143, loss = 0.30212169
Iteration 144, loss = 0.30452471
Iteration 145, loss = 0.30067833
Iteration 146, loss = 0.30179084
Iteration 147, loss = 0.30032160
Iteration 148, loss = 0.30070516
Iteration 149, loss = 0.30637083
Iteration 150, loss = 0.33555073
Iteration 151, loss = 0.30928236
Iteration 152, loss = 0.30152887
Iteration 153, loss = 0.29821684
Iteration 154, loss = 0.29779628
Iteration 155, loss = 0.29603874
Iteration 156, loss = 0.29656249
Iteration 157, loss = 0.29790388
Iteration 158, loss = 0.29760532
Iteration 159, loss = 0.29971038
Iteration 160, loss = 0.30072732
Iteration 161, loss = 0.29820566
Iteration 162, loss = 0.29554906
Iteration 163, loss = 0.29433218
Iteration 164, loss = 0.29411443
Iteration 165, loss = 0.29465920
Iteration 166, loss = 0.29864154
Iteration 167, loss = 0.29237515
Iteration 168, loss = 0.29342002
Iteration 169, loss = 0.29348368
Iteration 170, loss = 0.29104069
Iteration 171, loss = 0.29136262
Iteration 172, loss = 0.29546036
Iteration 173, loss = 0.29400210
Iteration 174, loss = 0.28935427
Iteration 175, loss = 0.29404600
Iteration 176, loss = 0.29080710
Iteration 177, loss = 0.28990259
Iteration 178, loss = 0.29064441
Iteration 179, loss = 0.29016912
Iteration 180, loss = 0.29078878
Iteration 181, loss = 0.29373419
Iteration 182, loss = 0.28910066
Iteration 183, loss = 0.28919647
Iteration 184, loss = 0.28932102
Iteration 185, loss = 0.28914971
Iteration 186, loss = 0.29027040
Iteration 187, loss = 0.29036524
Iteration 188, loss = 0.28759485
Iteration 189, loss = 0.28747122
Iteration 190, loss = 0.29001193
Iteration 191, loss = 0.29061881
Iteration 192, loss = 0.28731043
Iteration 193, loss = 0.28651163
Iteration 194, loss = 0.28702725
Iteration 195, loss = 0.28787661
Iteration 196, loss = 0.28797907
Iteration 197, loss = 0.28883871
Iteration 198, loss = 0.29213035
Iteration 199, loss = 0.29207838
Iteration 200, loss = 0.29373154
/opt/anaconda3/envs/audio_clf/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  warnings.warn(
-----training-----
precision_score:  1.0
recall_score:  0.15384615384615385
f1_score:  0.2666666666666667
accuracy_score:  0.9333838001514004
-----testing-----
/opt/anaconda3/envs/audio_clf/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
precision_score:  0.0
recall_score:  0.0
f1_score:  0.0
accuracy_score:  0.8880675818373812
--- 713.3026127815247 seconds ---