train: (5857, 27430)
test: (1204, 27430)
Positive labels in train:  524
Positive labels in test:  204
Iteration 1, loss = 41.04337647
Iteration 2, loss = 22.86939527
Iteration 3, loss = 12.22552279
Iteration 4, loss = 6.41109656
Iteration 5, loss = 3.37775207
Iteration 6, loss = 1.86593013
Iteration 7, loss = 1.14898087
Iteration 8, loss = 0.82879867
Iteration 9, loss = 0.67575749
Iteration 10, loss = 0.59513898
Iteration 11, loss = 0.56137857
Iteration 12, loss = 0.55887870
Iteration 13, loss = 0.53588225
Iteration 14, loss = 0.52190823
Iteration 15, loss = 0.50911651
Iteration 16, loss = 0.50164837
Iteration 17, loss = 0.49045343
Iteration 18, loss = 0.48108685
Iteration 19, loss = 0.47779713
Iteration 20, loss = 0.46625388
Iteration 21, loss = 0.46820932
Iteration 22, loss = 0.45751446
Iteration 23, loss = 0.44741820
Iteration 24, loss = 0.43762660
Iteration 25, loss = 0.45915500
Iteration 26, loss = 0.47244406
Iteration 27, loss = 0.44892395
Iteration 28, loss = 0.42052853
Iteration 29, loss = 0.41921311
Iteration 30, loss = 0.41169276
Iteration 31, loss = 0.40629928
Iteration 32, loss = 0.39936418
Iteration 33, loss = 0.39930024
Iteration 34, loss = 0.39394420
Iteration 35, loss = 0.38346017
Iteration 36, loss = 0.38288632
Iteration 37, loss = 0.38013374
Iteration 38, loss = 0.38007421
Iteration 39, loss = 0.37639849
Iteration 40, loss = 0.37331264
Iteration 41, loss = 0.36800621
Iteration 42, loss = 0.36954544
Iteration 43, loss = 0.36370489
Iteration 44, loss = 0.35837929
Iteration 45, loss = 0.35925337
Iteration 46, loss = 0.35403338
Iteration 47, loss = 0.35529386
Iteration 48, loss = 0.35051051
Iteration 49, loss = 0.34449622
Iteration 50, loss = 0.34194991
Iteration 51, loss = 0.34143246
Iteration 52, loss = 0.35214705
Iteration 53, loss = 0.34493961
Iteration 54, loss = 0.34980990
Iteration 55, loss = 0.33855226
Iteration 56, loss = 0.34373567
Iteration 57, loss = 0.33484170
Iteration 58, loss = 0.33307918
Iteration 59, loss = 0.32830403
Iteration 60, loss = 0.33298906
Iteration 61, loss = 0.32721320
Iteration 62, loss = 0.32736636
Iteration 63, loss = 0.32007658
Iteration 64, loss = 0.32023564
Iteration 65, loss = 0.32082939
Iteration 66, loss = 0.31790940
Iteration 67, loss = 0.31816625
Iteration 68, loss = 0.31290904
Iteration 69, loss = 0.31749934
Iteration 70, loss = 0.31398771
Iteration 71, loss = 0.31316046
Iteration 72, loss = 0.31270577
Iteration 73, loss = 0.31030786
Iteration 74, loss = 0.31024977
Iteration 75, loss = 0.30829995
Iteration 76, loss = 0.30778179
Iteration 77, loss = 0.30624575
Iteration 78, loss = 0.30655793
Iteration 79, loss = 0.30420619
Iteration 80, loss = 0.30572859
Iteration 81, loss = 0.30363115
Iteration 82, loss = 0.30451710
Iteration 83, loss = 0.30683473
Iteration 84, loss = 0.31229995
Iteration 85, loss = 0.30286493
Iteration 86, loss = 0.30186380
Iteration 87, loss = 0.30406237
Iteration 88, loss = 0.30112124
Iteration 89, loss = 0.30029975
Iteration 90, loss = 0.29726164
Iteration 91, loss = 0.30310812
Iteration 92, loss = 0.29866935
Iteration 93, loss = 0.29860058
Iteration 94, loss = 0.29568627
Iteration 95, loss = 0.29429324
Iteration 96, loss = 0.30340008
Iteration 97, loss = 0.29552932
Iteration 98, loss = 0.29710874
Iteration 99, loss = 0.29617728
Iteration 100, loss = 0.29671806
Iteration 101, loss = 0.29310824
Iteration 102, loss = 0.29642495
Iteration 103, loss = 0.29661984
Iteration 104, loss = 0.29704914
Iteration 105, loss = 0.29208282
Iteration 106, loss = 0.29111529
Iteration 107, loss = 0.29090020
Iteration 108, loss = 0.29146320
Iteration 109, loss = 0.29346921
Iteration 110, loss = 0.29098463
Iteration 111, loss = 0.29196586
Iteration 112, loss = 0.28835120
Iteration 113, loss = 0.30178837
Iteration 114, loss = 0.30561469
Iteration 115, loss = 0.29107638
Iteration 116, loss = 0.29064247
Iteration 117, loss = 0.28736543
Iteration 118, loss = 0.29012449
Iteration 119, loss = 0.29110026
Iteration 120, loss = 0.28874715
Iteration 121, loss = 0.28759088
Iteration 122, loss = 0.28839808
Iteration 123, loss = 0.28902256
Iteration 124, loss = 0.28876624
Iteration 125, loss = 0.28776861
Iteration 126, loss = 0.28719262
Iteration 127, loss = 0.28645394
Iteration 128, loss = 0.28633730
Iteration 129, loss = 0.28586110
Iteration 130, loss = 0.28602008
Iteration 131, loss = 0.28698386
Iteration 132, loss = 0.28365212
Iteration 133, loss = 0.28535877
Iteration 134, loss = 0.28532700
Iteration 135, loss = 0.28874409
Iteration 136, loss = 0.28716344
Iteration 137, loss = 0.28743742
Iteration 138, loss = 0.29236092
Iteration 139, loss = 0.28661401
Iteration 140, loss = 0.28417604
Iteration 141, loss = 0.29717920
Iteration 142, loss = 0.28876080
Iteration 143, loss = 0.28701191
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
-----training-----
precision_score:  0.9583333333333334
recall_score:  0.3511450381679389
f1_score:  0.5139664804469274
accuracy_score:  0.9405839166808947
-----testing-----
precision_score:  0.9166666666666666
recall_score:  0.10784313725490197
f1_score:  0.1929824561403509
accuracy_score:  0.8471760797342193
--- 4600.694417953491 seconds ---